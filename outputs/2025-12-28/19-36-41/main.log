[2025-12-28 19:36:41,307][__main__][INFO] - Loaded config: Multi-Model Pool (3x MPS)
[2025-12-28 19:36:41,307][src.server.services.orchestrator_service][INFO] - Tokenizer pool created: 3 workers, model: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:41,318][src.server.models.metrics.collector][INFO] - Experiment: Multi-Model Pool (3x MPS) | Backend: mps | Device: mps
[2025-12-28 19:36:41,318][src.server.services.orchestrator_service][INFO] - Using orchestrator wrapper for async tokenization -> inference flow
[2025-12-28 19:36:41,318][src.server.services.orchestrator_service][INFO] - Starting tokenization service...
[2025-12-28 19:36:42,359][src.server.services.tokenization_service][INFO] - Tokenizer loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:42,360][src.server.services.tokenization_service][INFO] - Tokenizer worker 1 loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:42,374][src.server.services.tokenization_service][INFO] - Tokenizer loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:42,389][src.server.services.tokenization_service][INFO] - Tokenizer worker 2 loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:42,389][src.server.services.tokenization_service][INFO] - Tokenizer loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:42,389][src.server.services.tokenization_service][INFO] - Tokenizer worker 0 loaded: cross-encoder/ms-marco-MiniLM-L-6-v2
[2025-12-28 19:36:43,324][src.server.services.tokenization_service][INFO] - Tokenizer pool ready with 3 workers
[2025-12-28 19:36:43,324][src.server.services.service_base][INFO] - TokenizationService started
[2025-12-28 19:36:43,325][src.server.services.orchestrator_service][INFO] - Starting inference service with 3 instances...
[2025-12-28 19:36:53,494][src.server.services.inference_service][INFO] - Pool ready with 3 workers
[2025-12-28 19:36:53,494][src.server.services.service_base][INFO] - InferenceService started
[2025-12-28 19:36:53,494][src.server.services.metrics_service][INFO] - MetricsService started with worker metrics collection (interval: 5.0s)
[2025-12-28 19:36:53,499][src.frontend.server][INFO] - Dashboard at http://localhost:8080
[2025-12-28 19:36:53,499][__main__][INFO] - Starting gRPC server on port 50051...
[2025-12-28 19:36:53,505][src.server.grpc][INFO] - gRPC server listening on 0.0.0.0:50051
[2025-12-28 19:36:55,461][src.server.models.metrics.collector][INFO] - Metrics reset
[2025-12-28 19:37:54,492][src.server.services.orchestrator_service][INFO] - Shutdown signal received
[2025-12-28 19:37:54,492][src.server.services.metrics_service][INFO] - MetricsService stopped
[2025-12-28 19:37:54,492][src.server.services.service_base][INFO] - InferenceService stopped
[2025-12-28 19:37:55,205][src.server.services.inference_service][INFO] - Pool stopped
[2025-12-28 19:37:55,205][src.server.services.service_base][INFO] - TokenizationService stopped
[2025-12-28 19:37:55,217][src.server.services.tokenization_service][INFO] - Tokenizer pool stopped
