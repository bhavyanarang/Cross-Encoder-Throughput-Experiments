# Experiment: MiniLM with MLX INT4 Quantization
name: "minilm_mlx_int4"
description: "MiniLM-L6-v2 with MLX backend and INT4 (4-bit) quantization on Apple Silicon"

model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "mps"
  backend: "mlx"
  mlx:
    bits: 4
    group_size: 64

# Note: MLX is Apple's native ML framework for Apple Silicon
# INT4 provides ~4x memory reduction with minimal accuracy loss
# Install: pip install mlx mlx-lm

