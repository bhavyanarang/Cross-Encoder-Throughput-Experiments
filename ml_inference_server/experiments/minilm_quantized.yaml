# Experiment: MiniLM FP16 Quantization (MPS)
name: "minilm_fp16"
description: "MiniLM-L6-v2 with FP16 half precision on MPS"

model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "mps"
  quantized: true
  quantization_mode: "fp16"
  backend: "pytorch"

# Note: FP16 is the best quantization option for MPS
# Expected: Similar or slightly better performance with lower memory

