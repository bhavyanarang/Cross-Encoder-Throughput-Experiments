server:
  host: "0.0.0.0"
  port: 50051

model:
  name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "mps"  # "mps", "cpu"
  quantized: false  # Enable quantization for the model

# Experiment parameters - adjust these for benchmarking
experiment:
  batch_sizes: [1, 4, 8, 16, 32]
  concurrency_levels: [1, 4, 8]
  warmup_iterations: 10
  benchmark_requests: 500

batching:
  enabled: false
  max_batch_size: 8
  timeout_ms: 100
