# Experiment 15: Production-Optimal Configuration
# Based on systematic experiment analysis - BEST RESULTS from Experiment 08a
#
# Key findings applied:
# 1. Dynamic batching with batch=64, conc=2 achieves 719 p/s @ 178ms latency
# 2. MPS backend performs identically to MLX (~520-530 p/s baseline)
# 3. Length-aware batching reduces padding waste by ~25%
# 4. Low timeout (20ms) maintains responsiveness
# 5. Concurrency=2 avoids MPS overload while enabling parallelism
#
# PRODUCTION RECOMMENDATION:
# This config achieves the best balance of throughput and latency:
# - Throughput: ~720 pairs/sec
# - Latency P50: ~176ms
# - Latency P99: ~211ms

name: "15_production_optimal"
description: "Production config: batch=64, conc=2, dynamic batching (from Exp 08a)"

model:
  name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  device: "mps"
  backend: "mps"  # MPS and MLX perform identically
  max_length: 256
  mps:
    fp16: true  # +10-12% throughput improvement

# Dynamic batching - critical for high throughput
batching:
  enabled: true
  max_batch_size: 64      # Optimal for throughput/latency balance
  timeout_ms: 20          # Low timeout for responsiveness
  length_aware_batching: true  # Reduce padding waste

# Server configuration
server:
  host: "0.0.0.0"
  port: 50051
  grpc_workers: 10

# Scheduler settings
scheduler:
  enable_length_aware_batching: true
  enable_stage_timing: true

# Experiment settings (for validation)
experiment:
  batch_sizes: [64]       # Optimal batch size
  concurrency_levels: [2]  # Optimal concurrency
  benchmark_requests: 200

# Performance targets (validated in Experiment 08a):
# - Throughput: 719.2 pairs/sec
# - Latency P50: 177.8ms
# - Latency P95: 201.7ms
# - Latency P99: 211.4ms
