# Experiment 16b: Dynamic Batching Analysis - Enabled
# Compare against 16a baseline to measure dynamic batching overhead
#
# Purpose: Measure throughput with dynamic batching to identify overhead.
# The scheduler aggregates incoming requests into batches up to max_batch_size
# or until timeout_ms expires, whichever comes first.
#
# HYPOTHESIS: Dynamic batching adds overhead from:
# 1. Thread synchronization (lock contention)
# 2. Queue management
# 3. Timeout waiting (even with requests available)
# 4. Batch assembly and result distribution

name: "16b_dynamic_batch_enabled"
description: "Dynamic batching enabled for overhead analysis"

model:
  backend: "mps"
  mps:
    fp16: true

# Dynamic batching ENABLED
batching:
  enabled: true
  max_batch_size: 256
  timeout_ms: 50
  length_aware_batching: true

experiment:
  # Match baseline for fair comparison
  batch_sizes: [32]
  concurrency_levels: [4]
  # Run for ~1 minute+ with 1000 requests
  benchmark_requests: 1000
