# Experiment 14: Optimized Production Configuration
# Based on analysis of all experiments - this combines the best settings found.
#
# Key findings applied:
# 1. MLX backend outperforms PyTorch by ~18%
# 2. Length-aware batching reduces latency by ~25%
# 3. Single model instance (no pool) is most stable on MPS
# 4. Concurrency=1 avoids MPS instability issues
# 5. Batch size 32-96 is the sweet spot for throughput/latency balance

name: "14_optimized_production"
description: "Production-optimized config: MLX + Length-Aware Batching"

model:
  name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  device: "mps"
  backend: "mlx"
  max_length: 256
  mlx:
    bits: 16
    group_size: 64

# Single-model mode (no pool) - pools are counter-productive on single GPU
# model_pool: not used

# No dynamic batching - static batching is more stable
batching:
  enabled: true
  max_batch_size: 256
  timeout_ms: 200

# Enable length-aware batching for reduced padding
scheduler:
  enable_length_aware_batching: true
  enable_stage_timing: true

# Test range of batch sizes to find optimal for your SLA
experiment:
  batch_sizes: [32, 48, 64, 96]
  concurrency_levels: [1]  # IMPORTANT: Keep at 1 to avoid MPS crashes

# Expected results (based on prior experiments):
# batch=32: ~700 p/s, ~46ms latency  (best latency)
# batch=48: ~750 p/s, ~60ms latency  (good balance)
# batch=64: ~780 p/s, ~80ms latency
# batch=96: ~800 p/s, ~120ms latency (best throughput)
