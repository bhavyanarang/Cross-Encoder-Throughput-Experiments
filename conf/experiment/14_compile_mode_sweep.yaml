# @package _global_

defaults:
  - override /model_pool: default
  - override /batching: default
  - override /tokenizer_pool: default
  - override /server: default

# Experiment 14: Compilation Mode - Sweep compile modes
name: "14_compile_mode_sweep"
description: "Compare torch.compile modes (default, reduce-overhead, max-autotune)"

# Compile mode sweep: will be expanded into multiple configs
# Note: compile_mode is handled specially - backend must be "compiled" and compile_model=true
model_pool:
  instances:
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: "compiled"
      device: "mps"
      quantization: "fp16"
      compile_model: true
      compile_mode: ["default", "reduce-overhead", "max-autotune"]
      max_length: 512

batching:
  enabled: false
  max_batch_size: 8
  timeout_ms: 100.0
  length_aware: false

experiment:
  batch_sizes: [64]
  concurrency_levels: [1]
