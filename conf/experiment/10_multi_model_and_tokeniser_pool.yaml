# @package _global_

defaults:
  - override /model_pool: default
  - override /batching: default
  - override /tokenizer_pool: default
  - override /server: default

# Multi-Model Pool with Tokenizer Pool Experiment
# Tests multiple model instances with round-robin routing and parallel tokenization

name: "Multi-Model Pool + Tokenizer Pool (2x MPS, 4 workers)"
description: "Two MPS model instances with round-robin routing and 4-worker tokenizer pool for parallel tokenization and inference"

# Multi-model pool configuration
model_pool:
  instances:
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: mps
      device: mps
      quantization: fp16
      compile_model: false
      max_length: 512
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: mps
      device: mps
      quantization: fp16
      compile_model: false
      max_length: 512
  routing_strategy: round_robin

# Tokenizer pool configuration
tokenizer_pool:
  enabled: true
  num_workers: 4
  model_name: ""  # If empty, uses same model as inference

batching:
  enabled: false
  max_batch_size: 32
  timeout_ms: 100.0
  length_aware: false

experiment:
  batch_sizes: [16, 32]
  concurrency_levels: [2, 4]
  warmup_iterations: 10
