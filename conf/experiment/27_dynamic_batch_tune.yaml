# @package _global_

defaults:
  - override /model_pool: default
  - override /batching: default
  - override /tokenizer_pool: default
  - override /server: default

name: "27_dynamic_batch_tune"
description: "Dynamic batch size sweep with 2x model pool and 4-worker tokenizer pool"

model_pool:
  instances:
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: "mps"
      device: "mps"
      quantization: "fp16"
      compile_model: false
      max_length: 512
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: "mps"
      device: "mps"
      quantization: "fp16"
      compile_model: false
      max_length: 512
  routing_strategy: "round_robin"

tokenizer_pool:
  enabled: true
  num_workers: 4

batching:
  enabled: true
  max_batch_size: [32, 64, 96]
  timeout_ms: 20.0
  length_aware: false

experiment:
  batch_sizes: [32, 64]
  concurrency_levels: [8, 16, 32]
  benchmark_requests: 500
  warmup_iterations: 10
