# @package _global_

batching:
  enabled: false
  length_aware: false
  max_batch_size: 64
  timeout_ms: 50.0
defaults:
- override /model_pool: default
- override /batching: default
- override /tokenizer_pool: default
- override /server: default
description: High-load backend sweep on Apple Silicon
experiment:
  batch_sizes:
  - 32
  - 64
  benchmark_requests: 500
  concurrency_levels:
  - 8
  - 16
  warmup_iterations: 10
model_pool:
  instances:
  - backend: pytorch
    compile_model: false
    device: mps
    max_length: 512
    name: cross-encoder/ms-marco-MiniLM-L-6-v2
    quantization: fp16
name: 24_backend_mps_sweep
tokenizer_pool:
  enabled: true
  num_workers: 1
