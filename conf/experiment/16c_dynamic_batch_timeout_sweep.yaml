# @package _global_

defaults:
  - override /model_pool: default
  - override /batching: default
  - override /tokenizer_pool: default
  - override /server: default

# Experiment 16c: Dynamic Batching - Timeout Sensitivity Analysis
# Test different timeout values to find optimal configuration
#
# Purpose: The timeout_ms parameter controls how long the scheduler waits
# to collect requests before processing a batch. Too high = latency overhead,
# too low = may not aggregate enough requests to benefit from batching.
#
# HYPOTHESIS: Lower timeouts should reduce overhead but may not aggregate
# enough requests under low load. Higher timeouts help under bursty load
# but add unnecessary latency under sustained load.

name: "16c_dynamic_batch_timeout_sweep"
description: "Timeout sensitivity analysis for dynamic batching"

model_pool:
  instances:
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      backend: "mps"
      device: "mps"
      quantization: fp16
      compile_model: false
      max_length: 512

# Dynamic batching with baseline timeout
batching:
  enabled: true
  max_batch_size: 64
  timeout_ms: 10.0  # Will be varied in experiment
  length_aware: false

# Scheduler settings
scheduler:
  enable_stage_timing: true

experiment:
  # Fixed batch size to isolate timeout impact
  batch_sizes: [32]
  # Higher concurrency to simulate realistic load
  concurrency_levels: [4]
  # Run for ~1 minute+ with 1000 requests
  benchmark_requests: 1000
