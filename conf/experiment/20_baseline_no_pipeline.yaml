# @package _global_

name: "Baseline Without Pipeline"
description: "Baseline experiment with sequential tokenization and inference (no pipeline)"

model_pool:
  instances:
    - name: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      device: "mps"
      backend: "pytorch"
      quantization: "fp32"
      max_length: 512

tokenizer_pool:
  num_workers: 2
  model_name: ""  # Empty means use same model as inference

batching:
  enabled: false

pipeline:
  enabled: false

server:
  http_port: 8080
  grpc_port: 50051
